**Multimodal Human-Robot Teaching Dataset**
This dataset captures multimodal interaction data collected during a study on teaching robots through human demonstration. The data includes eye tracking, motion tracking, robot trajectory, and subjective evaluations from participants performing structured teaching tasks under different feedback conditions.



**Experiment Overview**
Participants(N=28) were asked to perform Pick-and-Place and Sliding tasks using a teaching-by-demonstration interface in a controlled experimental setup. Each participant experienced one of four feedback conditions:
No Feedback-NF (Control)
Visual Feedback-VF
Descrptive Feedback-DF
Mixed Feedback-MF
Data was collected across multiple modalities to investigate how humans learn and teach robots, with an emphasis on usability, attention, and ergonomic performance.

participants.csv: Participant demographics (age, gender)
subjective_responses.csv: NASA-TLX and SUS questionnaire responses per condition and task
EyeTracking/: Gaze data and fixation metrics (participant → condition → task)
MotionTracking/: Motion capture data



**Use Case**
This dataset can support research in:
Human-robot interaction
Cognitive load estimation
Programming-by-demonstration systems
Eye tracking analysis
Multimodal machine learning

**License**
This dataset is shared under the Creative Commons Attribution 4.0 International (CC BY 4.0) license.


**Citation**
If you use this dataset, please cite our related data paper (DOI to be added soon).
